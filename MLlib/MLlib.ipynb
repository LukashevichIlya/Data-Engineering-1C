{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with MLLib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import all the necessary packages and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://d05198b55fe7:4040\n",
       "SparkContext available as 'sc' (version = 3.0.1, master = local[*], app id = local-1607725569467)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.mllib.classification.{SVMWithSGD, SVMModel}\n",
       "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
       "import org.apache.spark.mllib.util.MLUtils\n",
       "import org.apache.spark.mllib.linalg.Vectors\n",
       "import org.apache.spark.mllib.optimization.{LBFGS, LogisticGradient, SquaredL2Updater}\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.mllib.classification.{SVMWithSGD, SVMModel}\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.mllib.optimization.{LBFGS, LogisticGradient, SquaredL2Updater}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@12a5c7c1\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "      .builder\n",
    "      .appName(\"LinearSVC\")\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[6] at map at MLUtils.scala:86\n",
       "numFeatures: Int = 185316\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = MLUtils.loadLibSVMFile(sc, \"dataset.libsvm\")\n",
    "val numFeatures = data.take(1)(0).features.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SVMWithSGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model that will be used is SVMWithSVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "splits: Array[org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]] = Array(MapPartitionsRDD[7] at randomSplit at <console>:32, MapPartitionsRDD[8] at randomSplit at <console>:32)\n",
       "train_data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[7] at randomSplit at <console>:32\n",
       "test_data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[8] at randomSplit at <console>:32\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val splits = data.randomSplit(Array(0.7, 0.3), seed = 11L)\n",
    "val train_data = splits(0).cache()\n",
    "val test_data = splits(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numIterations: Int = 100\n",
       "svm_with_sgd: org.apache.spark.mllib.classification.SVMModel = org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures = 185316, numClasses = 2, threshold = 0.0\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numIterations = 100\n",
    "val svm_with_sgd = SVMWithSGD.train(train_data, numIterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: svm_with_sgd.type = org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures = 185316, numClasses = 2, threshold = None\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_with_sgd.clearThreshold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score_labels_sgd: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[212] at map at <console>:34\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val score_labels_sgd = test_data.map { point =>\n",
    "    val score = svm_with_sgd.predict(point.features)\n",
    "    (score, point.label)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metrics_sgd: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@40f267ef\n",
       "aucROC_sgd: Double = 0.40210812920343003\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val metrics_sgd = new BinaryClassificationMetrics(score_labels_sgd)\n",
    "val aucROC_sgd = metrics_sgd.areaUnderROC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC for SVM with SGD = 0.40210812920343003\n"
     ]
    }
   ],
   "source": [
    "println(s\"Area under ROC for SVM with SGD = $aucROC_sgd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using L-BFGS instead of SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply L-BFGS optimization algorithm instead of SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_data_lbfgs: org.apache.spark.rdd.RDD[(Double, org.apache.spark.mllib.linalg.Vector)] = MapPartitionsRDD[227] at map at <console>:32\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val train_data_lbfgs = splits(0).map(x => (x.label, MLUtils.appendBias(x.features))).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numCorrections: Int = 10\n",
       "convergenceTol: Double = 1.0E-4\n",
       "maxNumIterations: Int = 100\n",
       "regParam: Double = 0.1\n",
       "initialWeightsWithIntercept: org.apache.spark.mllib.linalg.Vector = [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,...\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numCorrections = 10\n",
    "val convergenceTol = 1e-4\n",
    "val maxNumIterations = 100\n",
    "val regParam = 0.1\n",
    "val initialWeightsWithIntercept = Vectors.dense(new Array[Double](numFeatures + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weightsWithIntercept: org.apache.spark.mllib.linalg.Vector = [0.0,-8.453627018328698E-8,-3.790526836658803E-6,-1.1038575527441863E-5,-9.57737544954984E-7,-9.71442139125112E-8,-2.0270416786332444E-5,-8.274583862475016E-5,-4.050405672202671E-6,-8.670932751760524E-6,0.0,-8.343250947621132E-6,-4.988237970347788E-7,2.7566262862127508E-5,-1.1034059455300929E-7,-2.6109600506465002E-6,-1.768503585250929E-7,0.0,-1.0295566332357653E-5,0.0,-2.3715478997203128E-7,-2.5091293643306628E-5,-7.446079864594622E-6,-1.1974778183093067E-5,-4.593376894059787E-5,-3.1380448783051945E-7,-6.126283798147832E-8,-1.3475873437764575E-6,-1.788447335690164E-7,-2.5889204345670688E-6,-3.765576875415585E-6,-1.0080012252339971E-5,-3.651889804009591E-5,-2.490306103124338E-6,0.0,-2.18351251219886E-5,-4.085157288299821E-7,0....\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val (weightsWithIntercept, loss) = LBFGS.runLBFGS(\n",
    "    train_data_lbfgs,\n",
    "    new LogisticGradient(),\n",
    "    new SquaredL2Updater(),\n",
    "    numCorrections,\n",
    "    convergenceTol,\n",
    "    maxNumIterations,\n",
    "    regParam,\n",
    "    initialWeightsWithIntercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svm_with_lbfgs: org.apache.spark.mllib.classification.SVMModel = org.apache.spark.mllib.classification.SVMModel: intercept = -0.18663851545993435, numFeatures = 185316, numClasses = 2, threshold = 0.0\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val svm_with_lbfgs = new SVMModel(\n",
    "    Vectors.dense(weightsWithIntercept.toArray.slice(0, weightsWithIntercept.size - 1)),\n",
    "    weightsWithIntercept(weightsWithIntercept.size - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: svm_with_lbfgs.type = org.apache.spark.mllib.classification.SVMModel: intercept = -0.18663851545993435, numFeatures = 185316, numClasses = 2, threshold = None\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_with_lbfgs.clearThreshold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score_labels_lbfgs: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[243] at map at <console>:34\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val score_labels_lbfgs = test_data.map { point =>\n",
    "    val score = svm_with_lbfgs.predict(point.features)\n",
    "    (score, point.label)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metrics_lbfgs: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@6cc94fb6\n",
       "aucROC_lbfgs: Double = 0.4475328488407433\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val metrics_lbfgs = new BinaryClassificationMetrics(score_labels_lbfgs)\n",
    "val aucROC_lbfgs = metrics_lbfgs.areaUnderROC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC for SVM with L-BFGS = 0.4475328488407433\n"
     ]
    }
   ],
   "source": [
    "println(s\"Area under ROC for SVM with L-BFGS = $aucROC_lbfgs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

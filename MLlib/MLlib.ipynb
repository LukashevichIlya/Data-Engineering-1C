{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with MLLib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import all the necessary packages and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://d7f1b9bc7dde:4040\n",
       "SparkContext available as 'sc' (version = 3.0.1, master = local[*], app id = local-1607800469041)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.mllib.classification.{SVMWithSGD, SVMModel}\n",
       "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
       "import org.apache.spark.mllib.util.MLUtils\n",
       "import org.apache.spark.mllib.linalg.Vectors\n",
       "import org.apache.spark.mllib.optimization.{LBFGS, LogisticGradient, SquaredL2Updater, HingeGradient}\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.mllib.classification.{SVMWithSGD, SVMModel}\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.mllib.optimization.{LBFGS, LogisticGradient, SquaredL2Updater, HingeGradient}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@50d34161\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "    .builder\n",
    "    .appName(\"LinearSVC\")\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[6] at map at MLUtils.scala:86\n",
       "numFeatures: Int = 185316\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = MLUtils.loadLibSVMFile(sc, \"dataset.libsvm\")\n",
    "val numFeatures = data.take(1)(0).features.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numEntries: Long = 149389\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numEntries = data.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SVMWithSGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model that will be used is SVMWithSVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "splits: Array[org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]] = Array(MapPartitionsRDD[7] at randomSplit at <console>:32, MapPartitionsRDD[8] at randomSplit at <console>:32)\n",
       "train_data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[7] at randomSplit at <console>:32\n",
       "test_data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[8] at randomSplit at <console>:32\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val splits = data.randomSplit(Array(0.7, 0.3), seed = 11L)\n",
    "val train_data = splits(0).cache()\n",
    "val test_data = splits(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numIterations: Int = 100\n",
       "svm_with_sgd: org.apache.spark.mllib.classification.SVMModel = org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures = 185316, numClasses = 2, threshold = 0.0\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numIterations = 100\n",
    "val svm_with_sgd = SVMWithSGD.train(train_data, numIterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: svm_with_sgd.type = org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures = 185316, numClasses = 2, threshold = None\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_with_sgd.clearThreshold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score_labels_sgd: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[212] at map at <console>:34\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val score_labels_sgd = test_data.map { point =>\n",
    "    val score = svm_with_sgd.predict(point.features)\n",
    "    if (score < 0.0) {\n",
    "        (0.0, point.label)\n",
    "    } else {\n",
    "        (1.0, point.label)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for SVM with SGD: 0.9990000444424693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "correct_counter: (Int, Int) = (44957,45002)\n",
       "acc: Double = 0.9990000444424693\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val correct_counter = score_labels_sgd\n",
    "    .map(x => if (x._1 == x._2) 1 else 0)\n",
    "    .aggregate((0, 0))(\n",
    "        (u, t) => (u._1 + t, u._2 + 1),\n",
    "        (u1, u2) => (u1._1 + u2._1, u1._2 + u2._2)\n",
    "    )\n",
    "\n",
    "val acc = correct_counter._1.toDouble / correct_counter._2\n",
    "println(s\"Accuracy for SVM with SGD: $acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score_labels_sgd: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[214] at map at <console>:34\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val score_labels_sgd = test_data.map { point =>\n",
    "    val score = svm_with_sgd.predict(point.features)\n",
    "    (score, point.label)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metrics_sgd: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@21e3801e\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val metrics_sgd = new BinaryClassificationMetrics(score_labels_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC = 0.40210812920341155\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "auROC: Double = 0.40210812920341155\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val auROC = metrics_sgd.areaUnderROC\n",
    "println(s\"Area under ROC = $auROC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using L-BFGS instead of SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply L-BFGS optimization algorithm instead of SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_data_lbfgs: org.apache.spark.rdd.RDD[(Double, org.apache.spark.mllib.linalg.Vector)] = MapPartitionsRDD[229] at map at <console>:32\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val train_data_lbfgs = splits(0).map(x => (x.label, MLUtils.appendBias(x.features))).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numCorrections: Int = 10\n",
       "convergenceTol: Double = 1.0E-4\n",
       "maxNumIterations: Int = 100\n",
       "regParam: Double = 0.1\n",
       "initialWeightsWithIntercept: org.apache.spark.mllib.linalg.Vector = [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,...\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numCorrections = 10\n",
    "val convergenceTol = 1e-4\n",
    "val maxNumIterations = 100\n",
    "val regParam = 0.1\n",
    "val initialWeightsWithIntercept = Vectors.dense(new Array[Double](numFeatures + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weightsWithIntercept: org.apache.spark.mllib.linalg.Vector = [0.0,6.960410288618664E-10,2.088123086585592E-9,4.176246173171184E-9,4.176246173171184E-9,1.3920820577237328E-9,-9.601829098911453E-5,2.6449559096750754E-8,6.960410288618664E-10,2.7841641154474657E-9,0.0,4.176246173171184E-9,2.088123086585592E-9,9.587890133624112E-5,6.960410288618664E-10,4.872287202033028E-9,6.960410288618664E-10,0.0,3.4802051443092993E-9,0.0,6.960410288618664E-10,4.176246173171184E-9,2.5997239735029336E-7,6.960410288618664E-10,-9.584131512068252E-5,6.960410288618664E-10,6.960410288618664E-10,6.960410288618664E-10,6.960410288618664E-10,2.7841641154474657E-9,6.960410288618664E-10,1.3920820577237328E-9,5.97416648615017E-7,6.960410288618664E-10,0.0,1.531290263496102E-8,2.088123086585592E-9,0.0,2.088123086585592E-...\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val (weightsWithIntercept, loss) = LBFGS.runLBFGS(\n",
    "    train_data_lbfgs,\n",
    "    new HingeGradient(),\n",
    "    new SquaredL2Updater(),\n",
    "    numCorrections,\n",
    "    convergenceTol,\n",
    "    maxNumIterations,\n",
    "    regParam,\n",
    "    initialWeightsWithIntercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svm_with_lbfgs: org.apache.spark.mllib.classification.SVMModel = org.apache.spark.mllib.classification.SVMModel: intercept = -0.1946827812723564, numFeatures = 185316, numClasses = 2, threshold = 0.0\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val svm_with_lbfgs = new SVMModel(\n",
    "    Vectors.dense(weightsWithIntercept.toArray.slice(0, weightsWithIntercept.size - 1)),\n",
    "    weightsWithIntercept(weightsWithIntercept.size - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: svm_with_lbfgs.type = org.apache.spark.mllib.classification.SVMModel: intercept = -0.1946827812723564, numFeatures = 185316, numClasses = 2, threshold = None\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_with_lbfgs.clearThreshold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score_labels_lbfgs: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[271] at map at <console>:34\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val score_labels_lbfgs = test_data.map { point =>\n",
    "    val score = svm_with_lbfgs.predict(point.features)\n",
    "    if (score < 0.0) {\n",
    "        (0.0, point.label)\n",
    "    } else {\n",
    "        (1.0, point.label)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Array[(Double, Double)] = Array((0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), ...\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_labels_lbfgs.take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for L-BFGS: 0.999022265677081\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "correct_counter: (Int, Int) = (44958,45002)\n",
       "acc: Double = 0.999022265677081\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val correct_counter = score_labels_lbfgs\n",
    "    .map(x => if (x._1 == x._2) 1 else 0)\n",
    "    .aggregate((0, 0))(\n",
    "        (u, t) => (u._1 + t, u._2 + 1),\n",
    "        (u1, u2) => (u1._1 + u2._1, u1._2 + u2._2)\n",
    "    )\n",
    "\n",
    "val acc = correct_counter._1.toDouble / correct_counter._2\n",
    "println(s\"Accuracy for L-BFGS: $acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score_labels_lbfgs: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[273] at map at <console>:34\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val score_labels_lbfgs = test_data.map { point =>\n",
    "    val score = svm_with_lbfgs.predict(point.features)\n",
    "    (score, point.label)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metrics_lbfgs: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@6df66d61\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val metrics_lbfgs = new BinaryClassificationMetrics(score_labels_lbfgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC = 0.5195197335695125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "auROC: Double = 0.5195197335695125\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val auROC = metrics_lbfgs.areaUnderROC\n",
    "println(s\"Area under ROC = $auROC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is very unbalanced and we see that models learn to predict only one class. Therefore, their accuracy is almost perfect, while roc-auc score is not. I think that in this task it is more suitable to use L-BFGS, because the data has a lot of features (185316), which is even greater than the number of entries in the dataset and L-BFGS migth be more stable and less noisy because of its Hessian approximation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Схема данных\n",
    "\n",
    "| customer               |                                                                                                                                                       |\n",
    "|------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| customer_id            | Идентификатор клиента                                                                                                                                 |\n",
    "| product_X              | Статус продукта. OPN - открыт, но не утилизирован. UTL - утилизирован. CLS - закрыт                                                                   |\n",
    "| gender_cd              | Пол. M - мужской. F - женский                                                                                                                         |\n",
    "| age                    | Возраст в годах                                                                                                                                       |\n",
    "| marital_status_cd      | Семейный статус. См. словарь соответствия                                                                                                             |\n",
    "| children_cnt           | Количество детей в штуках                                                                                                                             |\n",
    "| first_session_dttm     | Дата и время первой сессии в приложении или личном кабинете на сайте                                                                                  |\n",
    "| job_position_cd        | Категория занимаемой должности. См. словарь соответствия                                                                                              |\n",
    "| job_title              | Занимаемая должность                                                                                                                                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| stories_reaction_train |                                                                                                                                                       |\n",
    "|------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| customer_id            | Идентификатор клиента                                                                                                                                 |\n",
    "| story_id               | Идентификатор истории                                                                                                                                 |\n",
    "| event_dttm             | Дата действия                                                                                                                                         |\n",
    "| event                  | Тип действия. like - лайк или сохранение. view - глубокий просмотр (более 10 секунд). skip - пролистанная история (менее 5 секунд). dislike - дизлайк |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://2f03deb27994:4040\n",
       "SparkContext available as 'sc' (version = 3.0.1, master = local[*], app id = local-1603576737684)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.rdd._\n",
       "defined class SimpleCSVHeader\n",
       "csvCustomer: org.apache.spark.rdd.RDD[String] = ./customer_train.csv MapPartitionsRDD[1] at textFile at <console>:31\n",
       "dataCustomer: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[2] at map at <console>:32\n",
       "headerCustomer: SimpleCSVHeader = SimpleCSVHeader@375a6cce\n",
       "customers: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[3] at filter at <console>:34\n",
       "csvStories: org.apache.spark.rdd.RDD[String] = ./stories_reaction_train.csv MapPartitionsRDD[5] at textFile at <console>:36\n",
       "dataStories: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[6] at map at <console>:37\n",
       "headerStories: SimpleCSVHeader = SimpleCSVHeader@624d0cd1\n",
       "stories: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[7] at filter at...\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.rdd._\n",
    "\n",
    "class SimpleCSVHeader(header:Array[String]) extends Serializable {\n",
    "  val index = header.zipWithIndex.toMap\n",
    "  def apply(array:Array[String], key:String) : String = {\n",
    "      val curIndex = index(key)\n",
    "      if (curIndex < array.size) {\n",
    "          return array(curIndex)\n",
    "      } else {\n",
    "          return \"\"\n",
    "      }\n",
    "  }\n",
    "}\n",
    "\n",
    "val csvCustomer = sc.textFile(\"./customer_train.csv\")  // original file\n",
    "val dataCustomer = csvCustomer.map(line => line.split(\",\").map(elem => elem.trim)) //lines in rows\n",
    "val headerCustomer = new SimpleCSVHeader(dataCustomer.first()) // we build our header with the first line\n",
    "val customers = dataCustomer.filter(line => headerCustomer(line,\"customer_id\") != \"customer_id\") // filter the header out\n",
    "\n",
    "val csvStories = sc.textFile(\"./stories_reaction_train.csv\")  // original file\n",
    "val dataStories = csvStories.map(line => line.split(\",\").map(elem => elem.trim)) //lines in rows\n",
    "val headerStories = new SimpleCSVHeader(dataStories.first()) // we build our header with the first line\n",
    "val stories = dataStories.filter(line => headerStories(line,\"customer_id\") != \"customer_id\") // filter the header out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Посчитать количество пользователей без детей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "counterNoChildren: Int = 37284\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val counterNoChildren = customers\n",
    "    .map(s => if (headerCustomer(s, \"children_cnt\") == \"0.0\") 1 else 0)\n",
    "    .aggregate(0)(\n",
    "        (u, t) => u + t,\n",
    "        (u1, u2) => u1 + u2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Посчитать долю пользователей старше 40 лет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "counterOlder40: (Int, Int) = (8425,50000)\n",
       "res0: Double = 0.1685\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val counterOlder40 = customers\n",
    "    .map(s => if (headerCustomer(s, \"age\") > \"40.0\") 1 else 0)\n",
    "    .aggregate((0, 0))(\n",
    "        (u, t) => (u._1 + t, u._2 + 1),\n",
    "        (u1, u2) => (u1._1 + u2._1, u1._2 + u2._2)\n",
    "    )\n",
    "\n",
    "counterOlder40._1.toDouble / counterOlder40._2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Посчитать количество историй, которые лайкнули люди, утилизировавшие продукт 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customersProduct2: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[10] at map at <console>:33\n",
       "res1: Long = 531\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val customersProduct2 = customers\n",
    "    .map(s => (headerCustomer(s, \"customer_id\"), headerCustomer(s, \"product_2\")))\n",
    "\n",
    "stories\n",
    "    .filter(s => headerStories(s, \"event\") == \"like\")\n",
    "    .map(s => (headerStories(s, \"customer_id\"), headerStories(s, \"story_id\")))\n",
    "    .join(customersProduct2)\n",
    "    .map(s => (s._2._1, if (s._2._2 == \"UTL\") 1 else 0))\n",
    "    .aggregateByKey((0, 0))(\n",
    "        (u, t) => (u._1 + t, u._2 + 1),\n",
    "        (u1, u2) => (u1._1 + u2._1, u1._2 + u2._2)\n",
    "    )\n",
    "    .filter(s => s._2._1 != 0)\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. С помощью flatMap посчитать у каждого пользователя количество продуктов по статусу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "counterByProductStatus: Array[((String, String), Int)] = Array(((120452,CLS),1), ((481318,\"\"),6), ((509622,\"\"),6), ((173764,\"\"),6), ((250131,\"\"),5), ((499135,UTL),2), ((280846,OPN),1), ((195435,\"\"),6), ((43842,\"\"),6), ((46645,UTL),1))\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// На выходе получаем записи вида ((customer_id, product_status), counter_by_product_status)\n",
    "\n",
    "val counterByProductStatus = customers\n",
    "    .flatMap(s =>\n",
    "             Seq((headerCustomer(s, \"customer_id\"), headerCustomer(s, \"product_0\"), 1),\n",
    "                 (headerCustomer(s, \"customer_id\"), headerCustomer(s, \"product_1\"), 1),\n",
    "                 (headerCustomer(s, \"customer_id\"), headerCustomer(s, \"product_2\"), 1),\n",
    "                 (headerCustomer(s, \"customer_id\"), headerCustomer(s, \"product_3\"), 1),\n",
    "                 (headerCustomer(s, \"customer_id\"), headerCustomer(s, \"product_4\"), 1),\n",
    "                 (headerCustomer(s, \"customer_id\"), headerCustomer(s, \"product_5\"), 1),\n",
    "                 (headerCustomer(s, \"customer_id\"), headerCustomer(s, \"product_6\"), 1)\n",
    "             )\n",
    "    )\n",
    "    .map(s => ((s._1, s._2), s._3))\n",
    "    .aggregateByKey(0)(\n",
    "        (u, t) => u + t,\n",
    "        (u1, u2) => u1 + u2\n",
    "    )\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Определить даты, в которые была наибольшая и наименьшая доля лайков историй от мужчин"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genderRDD: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[22] at map at <console>:31\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val genderRDD = customers\n",
    "    .map(s => (headerCustomer(s, \"customer_id\"), headerCustomer(s, \"gender_cd\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datesAndMenLikes: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[28] at map at <console>:38\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datesAndMenLikes = stories\n",
    "    .filter(s => headerStories(s, \"event\") == \"like\")\n",
    "    .map(s => (headerStories(s, \"customer_id\"), \n",
    "               (headerStories(s, \"story_id\"), \n",
    "                headerStories(s, \"event_dttm\").slice(0, 10))))\n",
    "    .join(genderRDD)\n",
    "    .map(s => (s._2._1._2, if (s._2._2 == \"M\") 1 else 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datesLikesRatio: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[30] at map at <console>:34\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datesLikesRatio = datesAndMenLikes\n",
    "    .aggregateByKey((0, 0))(\n",
    "        (u, t) => (u._1 + t, u._2 + 1),\n",
    "        (u1, u2) => (u1._1 + u2._1, u1._2 + u2._2)\n",
    "    )\n",
    "    .map(s => (s._1, s._2._1.toDouble / s._2._2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "maxKey: Array[(String, Double)] = Array((2018-06-29,0.9574468085106383))\n",
       "minKey: Array[(String, Double)] = Array((2018-07-29,0.6534653465346535))\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val maxKey = datesLikesRatio.takeOrdered(1)(Ordering[Double].reverse.on(_._2))\n",
    "val minKey = datesLikesRatio.takeOrdered(1)(Ordering[Double].on(_._2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Среди тех, кто посмотрел историю 138, найдите id пользователя с максимальным количеством детей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "childrenRDD: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[33] at map at <console>:36\n",
       "n_ids: Int = 5\n",
       "ids: Array[(String, String)] = Array((233438,3.0), (178091,3.0), (172754,3.0), (189145,3.0), (378078,3.0))\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// В данных условиях задачи получается, что у нескольких id пользователя максимальное количество детей (3 детей), \n",
    "// поэтому можно вывести несколько id с соответствующим количеством детей\n",
    "\n",
    "val childrenRDD = customers\n",
    "    .map(s => (headerCustomer(s, \"customer_id\"), headerCustomer(s, \"children_cnt\")))\n",
    "\n",
    "val n_ids = 5\n",
    "\n",
    "val ids = stories\n",
    "    .filter(s => headerStories(s, \"story_id\") == \"138\")\n",
    "    .map(s => (headerStories(s, \"customer_id\"), 1))\n",
    "    .join(childrenRDD)\n",
    "    .takeOrdered(n_ids)(Ordering[String].reverse.on(_._2._2))\n",
    "    .map(s => (s._1, s._2._2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Найдите id истории с наибольшим отношением skip'ов к like'ам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idAndRatio: Array[(String, Double)] = Array((171,643.0))\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// В данных очень мало случаев, когда \"event\" == \"like\", поэтому давайте отфильтруем те истории, для которых\n",
    "// количество лайков равно нулю\n",
    "\n",
    "val idAndRatio = stories\n",
    "    .map(s => (headerStories(s, \"story_id\"), \n",
    "               (if (headerStories(s, \"event\") == \"skip\") 1 else 0,\n",
    "                if (headerStories(s, \"event\") == \"like\") 1 else 0)\n",
    "              )\n",
    "    )\n",
    "    .aggregateByKey((0, 0))(\n",
    "        (u, t) => (u._1 + t._1, u._2 + t._2),\n",
    "        (u1, u2) => (u1._1 + u2._1, u1._2 + u2._2)\n",
    "    )\n",
    "    .filter(s => s._2._2 != 0)\n",
    "    .map(s => (s._1, s._2._1.toDouble / s._2._2))\n",
    "    .takeOrdered(1)(Ordering[Double].reverse.on(_._2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Напишите tail recursive функцию конкатенации листа листов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.annotation.tailrec\n",
       "flattenTailRec: (list: List[Any])List[Any]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.annotation.tailrec\n",
    "\n",
    "def flattenTailRec(list: List[Any]): List[Any] = {\n",
    "    @tailrec\n",
    "    def flatten(res: List[Any], rem: List[Any]): List[Any] = rem match {\n",
    "        case Nil => res\n",
    "        case (h: List[_]) :: Nil => flatten(res, h)\n",
    "        case (h: List[_]) :: tail => flatten(res ::: flattenTailRec(h), tail)\n",
    "        case h :: tail => flatten(res ::: List(h), tail)\n",
    "    }\n",
    "    flatten(List(), list)\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list: List[List[Any]] = List(List(1, 1), List(1, List(5, 10), 2, 3), List(1))\n",
       "flattenedList: List[Any] = List(1, 1, 1, 5, 10, 2, 3, 1)\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val list = List(List(1, 1), List(1, List(5, 10), 2, 3), List(1))\n",
    "val flattenedList = flattenTailRec(list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

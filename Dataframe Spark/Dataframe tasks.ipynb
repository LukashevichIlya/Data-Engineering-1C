{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Читаем данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://7024ae5bbc52:4040\n",
       "SparkContext available as 'sc' (version = 3.0.1, master = local[*], app id = local-1604926017613)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.expressions._\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@35157d6d\n",
       "recipes: org.apache.spark.sql.DataFrame = [name: string, id: int ... 10 more fields]\n",
       "interactions: org.apache.spark.sql.DataFrame = [user_id: int, recipe_id: int ... 3 more fields]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.expressions._\n",
    "\n",
    "val spark = org.apache.spark.sql.SparkSession.builder\n",
    "        .master(\"local\")\n",
    "        .appName(\"Spark CSV Reader\")\n",
    "        .getOrCreate;\n",
    "\n",
    " val recipes = spark.read\n",
    "        .format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(\"recipes.csv\")\n",
    "\n",
    " val interactions = spark.read\n",
    "        .format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(\"interactions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Посмотрите на данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- minutes: integer (nullable = true)\n",
      " |-- contributor_id: integer (nullable = true)\n",
      " |-- submitted: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- nutrition: string (nullable = true)\n",
      " |-- n_steps: integer (nullable = true)\n",
      " |-- steps: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- ingredients: string (nullable = true)\n",
      " |-- n_ingredients: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recipes.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-------+--------------+----------+--------------------+--------------------+-------+--------------------+--------------------+--------------------+-------------+\n",
      "|                name|    id|minutes|contributor_id| submitted|                tags|           nutrition|n_steps|               steps|         description|         ingredients|n_ingredients|\n",
      "+--------------------+------+-------+--------------+----------+--------------------+--------------------+-------+--------------------+--------------------+--------------------+-------------+\n",
      "|arriba   baked wi...|137739|     55|         47892|2005-09-16|['60-minutes-or-l...|[51.5, 0.0, 13.0,...|     11|['make a choice a...|autumn is my favo...|['winter squash',...|            7|\n",
      "+--------------------+------+-------+--------------+----------+--------------------+--------------------+-------+--------------------+--------------------+--------------------+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recipes.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- recipe_id: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- review: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interactions.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+------+--------------------+\n",
      "|user_id|recipe_id|      date|rating|              review|\n",
      "+-------+---------+----------+------+--------------------+\n",
      "|  38094|    40893|2003-02-17|     4|Great with a sala...|\n",
      "|1293707|    40893|2011-12-21|     5|So simple, so del...|\n",
      "|   8937|    44394|2002-12-01|     4|This worked very ...|\n",
      "| 126440|    85009|2010-02-27|     5|I made the Mexica...|\n",
      "|  57222|    85009|2011-10-01|     5|Made the cheddar ...|\n",
      "+-------+---------+----------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interactions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Найдите пользователя с максимальным количеством отзывов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "max_reviews_user: Array[org.apache.spark.sql.Row] = Array([424680,7671])\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val max_reviews_user = interactions\n",
    "    .groupBy(\"user_id\")\n",
    "    .agg(count(\"*\") as \"reviews_count\")\n",
    "    .orderBy(-$\"reviews_count\")\n",
    "    .take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Найдите пользователя, который потратил больше всех времени на готовку (придумайте как это примерно посчитать с имеющимися данными)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данных есть рецепт с id, равным 261647, и временем изготовления в 2147483647 минут (максимальный Int), что кажется странным. Поэтому отфильтруем этот рецепт. Будем считать, что пользователь суммарное потраченное пользователем время на готовку равно сумме всех времен на готовку из рецептов, по которым пользователь оставлял оценки или отзывы (из таблички interactions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "recipesWithTime: org.apache.spark.sql.DataFrame = [recipe_id: int, minutes: int]\n",
       "max_cooking_time_user: Array[org.apache.spark.sql.Row] = Array([1072593,1296802])\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val recipesWithTime = recipes\n",
    "    .filter($\"id\" !== 261647)\n",
    "    .select($\"id\" as \"recipe_id\", $\"minutes\")\n",
    "\n",
    "val max_cooking_time_user = interactions\n",
    "    .select($\"user_id\", $\"recipe_id\")\n",
    "    .join(recipesWithTime, Seq(\"recipe_id\"))\n",
    "    .groupBy(\"user_id\")\n",
    "    .agg(sum($\"minutes\") as \"total_cooking_time\")\n",
    "    .orderBy(-$\"total_cooking_time\")\n",
    "    .take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Найдите пользователя, который дольше всего пользуется сайтом (придумайте как это примерно посчитать с имеющимися данными)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не до конца понятно, что подразумевается под наиболее долгим пользованием сайтом. Посмотрим на самые ранние добавления рецептов от пользователей и на самые ранние взаимодействия пользователей (по таблицам recipes и interactions соответственно). Будем считать, что кто раньше всех \"оставил свой след\", тот и дольше всего пользуется сайтом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------------+\n",
      "|contributor_id|earliest_contribution|\n",
      "+--------------+---------------------+\n",
      "|          1571|  1999-08-06 00:00:00|\n",
      "|          1543|  1999-08-06 00:00:00|\n",
      "|          1533|  1999-08-06 00:00:00|\n",
      "|          1580|  1999-08-06 00:00:00|\n",
      "|        115621|  1999-08-06 00:00:00|\n",
      "|          1535|  1999-08-06 00:00:00|\n",
      "|         39547|  1999-08-07 00:00:00|\n",
      "|          1587|  1999-08-07 00:00:00|\n",
      "|          1604|  1999-08-07 00:00:00|\n",
      "|        174711|  1999-08-08 00:00:00|\n",
      "+--------------+---------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "earliest_contributions: Unit = ()\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val earliest_contributions = recipes\n",
    "    .select($\"contributor_id\", to_timestamp($\"submitted\") as \"date\")\n",
    "    .groupBy(\"contributor_id\")\n",
    "    .agg(min($\"date\") as \"earliest_contribution\")\n",
    "    .orderBy($\"earliest_contribution\")\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|user_id|earliest_interaction|\n",
      "+-------+--------------------+\n",
      "|   2008| 2000-01-25 00:00:00|\n",
      "|   2046| 2000-02-25 00:00:00|\n",
      "|   1773| 2000-03-13 00:00:00|\n",
      "|   2156| 2000-06-02 00:00:00|\n",
      "|   1986| 2000-08-26 00:00:00|\n",
      "|   2585| 2000-09-05 00:00:00|\n",
      "|   2625| 2000-09-11 00:00:00|\n",
      "|   2312| 2000-09-12 00:00:00|\n",
      "|   2536| 2000-09-12 00:00:00|\n",
      "|   2033| 2000-09-20 00:00:00|\n",
      "+-------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "earliest_interactions: Unit = ()\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val earliest_interactions = interactions\n",
    "    .select($\"user_id\", to_timestamp($\"date\") as \"date\")\n",
    "    .groupBy(\"user_id\")\n",
    "    .agg(min($\"date\") as \"earliest_interaction\")\n",
    "    .orderBy($\"earliest_interaction\")\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что самые ранние добавления рецептов были раньше, чем самые ранние взаимодействия, поэтому результат будет таковым:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "longest_time_user: Array[org.apache.spark.sql.Row] = Array([1580,1999-08-06 00:00:00.0])\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val longest_time_user = recipes\n",
    "    .select($\"contributor_id\", to_timestamp($\"submitted\") as \"date\")\n",
    "    .groupBy(\"contributor_id\")\n",
    "    .agg(min($\"date\") as \"earliest_contribution\")\n",
    "    .orderBy($\"earliest_contribution\")\n",
    "    .take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Найдите пользователя, который берёт только новые рецепты (придумайте как это примерно посчитать с имеющимися данными)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем считать определенную дату за барьер, после которого рецепты считаются новыми. А сравнивать пользователей будем по отношению новых рецептов ко всем, которыми он \"пользовался\" (оставил отзыв, т.е. провзаимодейстовал)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- minutes: integer (nullable = true)\n",
      " |-- contributor_id: integer (nullable = true)\n",
      " |-- submitted: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- nutrition: string (nullable = true)\n",
      " |-- n_steps: integer (nullable = true)\n",
      " |-- steps: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- ingredients: string (nullable = true)\n",
      " |-- n_ingredients: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- recipe_id: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- review: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recipes.printSchema\n",
    "interactions.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------------+-----------+\n",
      "|interaction_user_id|total_interacted_recipes|new_recipes|\n",
      "+-------------------+------------------------+-----------+\n",
      "|         2002276513|                       1|          1|\n",
      "|            2389450|                       1|          1|\n",
      "|         2002026576|                       1|          1|\n",
      "|         2002326807|                       1|          1|\n",
      "|         2002053137|                       1|          1|\n",
      "|         2002259490|                       1|          1|\n",
      "|         2001842075|                       1|          1|\n",
      "|         1802507836|                       1|          1|\n",
      "|         2002277099|                       1|          1|\n",
      "|         2001936405|                       1|          1|\n",
      "|         2001900143|                       1|          1|\n",
      "|         2002162063|                       1|          1|\n",
      "|         2002203938|                       1|          1|\n",
      "|         2002225247|                       1|          1|\n",
      "|            1633565|                       1|          1|\n",
      "|         2002248640|                       1|          1|\n",
      "|         2001914022|                       1|          1|\n",
      "|         2002212283|                       1|          1|\n",
      "|         2002362042|                       1|          1|\n",
      "|         2001664226|                       1|          1|\n",
      "+-------------------+------------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "usersRecipes: org.apache.spark.sql.DataFrame = [interaction_user_id: int, recipe_id: int]\n",
       "result: Unit = ()\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val usersRecipes = interactions\n",
    "    .select($\"user_id\" as \"interaction_user_id\", $\"recipe_id\")\n",
    "\n",
    "val result = recipes\n",
    "    .select($\"id\" as \"recipe_id\", to_timestamp($\"submitted\") as \"submitted\")\n",
    "    .join(usersRecipes, Seq(\"recipe_id\"))\n",
    "    .groupBy(\"interaction_user_id\")\n",
    "    .agg(\n",
    "        count(\"*\") as \"total_interacted_recipes\",\n",
    "        sum(when(datediff($\"submitted\", to_timestamp(lit(\"2018-01-01\"))) > 1, lit(1)).otherwise(lit(0))) as \"new_recipes\"\n",
    "    )\n",
    "    .orderBy(-($\"new_recipes\".cast(\"double\") / $\"total_interacted_recipes\"))\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что в топе полученной таблицы в основном пользователи с одним отзывом, который сделан на новый рецепт. Наверное возможно использовать какой-то другой способ сравнения, но я выбрал этот."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Найдите пользователя, который выбирает самые сложные рецепты. Метрика: 0.2 * (медианное количество шагов в выбранном рецепте) + 0.3 * (среднее количество ингредиентов) + (средняя длина описания)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем считать среднюю метрику по всем рецептам пользователя для решения задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "recipesExtended: org.apache.spark.sql.DataFrame = [name: string, id: int ... 13 more fields]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val recipesExtended = recipes\n",
    "    .withColumn(\"median_steps\", callUDF(\"percentile_approx\", col(\"n_steps\"), lit(0.5)).over(Window.partitionBy($\"name\")))\n",
    "    .withColumn(\"avg_ingredients\", avg($\"n_ingredients\".cast(\"int\")).over(Window.partitionBy($\"name\")))\n",
    "    .withColumn(\"avg_desc_len\", avg(length($\"description\")).over(Window.partitionBy($\"name\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userComplicatedRecipes: Array[org.apache.spark.sql.Row] = Array([1802742723,6323.4])\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val userComplicatedRecipes = recipesExtended\n",
    "    .select($\"id\" as \"recipe_id\", $\"name\", $\"median_steps\", $\"avg_ingredients\", $\"avg_desc_len\")\n",
    "    .join(usersRecipes, Seq(\"recipe_id\"))\n",
    "    .withColumn(\"complexity_metric\", \n",
    "                lit(0.2) * when($\"median_steps\".isNull, lit(0)).otherwise($\"median_steps\")\n",
    "                + lit(0.3) * when($\"avg_ingredients\".isNull, lit(0)).otherwise($\"avg_ingredients\")\n",
    "                + when($\"avg_desc_len\".isNull, lit(0)).otherwise($\"avg_desc_len\")\n",
    "    )\n",
    "    .groupBy(\"interaction_user_id\")\n",
    "    .agg(avg($\"complexity_metric\") as \"avg_complexity\")\n",
    "    .orderBy(-$\"avg_complexity\")\n",
    "    .take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Найдите количество пользователей, которые выбирают наиболее разнообразную пищу. 2 вариации:\n",
    "    1. На основании тэгов\n",
    "    2. На основании лемматизированных описаний и TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`A.` Считаем, что чем больше различных тэгов использует пользователь, тем более разнообразную пищу он выбирает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unqTags: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [user_id: int, n_unq_tags: bigint]\n",
       "maxNumTags: Any = 643\n"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val unqTags = recipes\n",
    "    .select($\"id\" as \"recipe_id\", explode(split(expr(\"substring(tags, 2, length(tags) - 1)\"), \",\")) as \"tags\")\n",
    "    .join(interactions.select(\"user_id\", \"recipe_id\"), Seq(\"recipe_id\"))\n",
    "    .groupBy(\"user_id\")\n",
    "    .agg(countDistinct($\"tags\") as \"n_unq_tags\")\n",
    "    .orderBy(-$\"n_unq_tags\")\n",
    "    .cache\n",
    "\n",
    "val maxNumTags = unqTags.select(max(\"n_unq_tags\")).take(1)(0)(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------+\n",
      "|user_id|n_unq_tags|max_num_tags|\n",
      "+-------+----------+------------+\n",
      "| 424680|       643|         643|\n",
      "+-------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unqTags\n",
    "    .withColumn(\"max_num_tags\", lit(maxNumTags))\n",
    "    .filter($\"n_unq_tags\" === $\"max_num_tags\")\n",
    "    .show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. \n",
    "\n",
    "\n",
    "A. Найдите медианное значение калорийности блюд, приготовленных пользователями с более чем 5-ю отзывами\n",
    "\n",
    "B. В качестве веса в медиане используйте количество отзывов на блюдо. Взвешенную медиану необходимо реализовать через оконные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Найдите слова в отзыве, которые наиболее коррелируют с хорошим рейтингом (4-5). Отфильтруйте слишком редкие слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------+\n",
      "|               word|freq_in_high_rating|cnt_word|\n",
      "+-------------------+-------------------+--------+\n",
      "|          honkitten|                1.0|     348|\n",
      "|         keepermade|                1.0|     177|\n",
      "|           gailanng|                1.0|     222|\n",
      "|      perfectthanks|                1.0|     177|\n",
      "|wholemealwholegrain|                1.0|     187|\n",
      "|           adoptees|  0.998158379373849|    1086|\n",
      "|         aussiekiwi| 0.9960474308300395|     253|\n",
      "|          againmade| 0.9957264957264957|     234|\n",
      "|             boomie| 0.9956709956709957|     231|\n",
      "|          travelers| 0.9955555555555555|     225|\n",
      "|          recipenap| 0.9952380952380953|     210|\n",
      "|               pets| 0.9951923076923077|     208|\n",
      "|               kitz|  0.995049504950495|     202|\n",
      "|              katie| 0.9948717948717949|     195|\n",
      "|            dragons| 0.9942528735632183|     174|\n",
      "|            witchin| 0.9938650306748467|     163|\n",
      "|           nibbling| 0.9937888198757764|     161|\n",
      "|        participant| 0.9936708860759493|     158|\n",
      "|    vegetarianvegan| 0.9935064935064936|     308|\n",
      "|         daredevils| 0.9933333333333333|     150|\n",
      "+-------------------+-------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "min_freq: Int = 150\n",
       "wordsRatings: Unit = ()\n"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val min_freq = 150\n",
    "\n",
    "val wordsRatings = interactions\n",
    "    .select($\"rating\", explode(split(lower(regexp_replace($\"review\", \"[^a-zA-Z ]\", \"\")), \" \")) as \"word\")\n",
    "    .filter($\"word\" !== \"\")\n",
    "    .withColumn(\"cnt_word\", count(\"*\") over Window.partitionBy($\"word\"))\n",
    "    .filter($\"cnt_word\" >= min_freq && $\"rating\" >= 4)\n",
    "    .withColumn(\"cnt_word_with_high_rating\", count(\"*\") over Window.partitionBy($\"word\"))\n",
    "    .drop(\"rating\")\n",
    "    .distinct\n",
    "    .select($\"word\", $\"cnt_word_with_high_rating\" / $\"cnt_word\" as \"freq_in_high_rating\", $\"cnt_word\")\n",
    "    .orderBy(-$\"freq_in_high_rating\")\n",
    "    .show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Найдите теги, блюда с которыми имеют в среднем худшие отзывы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|                 tag|        avg_rating|\n",
      "+--------------------+------------------+\n",
      "|    shrimp-main-dish|               0.0|\n",
      "|lamb-sheep-main-dish|               0.0|\n",
      "|           pot-roast|               0.0|\n",
      "|   main-dish-seafood|               0.0|\n",
      "|      beef-crock-pot|               0.0|\n",
      "|           bean-soup|               3.0|\n",
      "|      main-dish-beef|               3.0|\n",
      "|     black-bean-soup|               3.0|\n",
      "|       stews-poultry|3.3333333333333335|\n",
      "|                bear|3.6666666666666665|\n",
      "|    pressure-canning|3.6742424242424243|\n",
      "|       sugar-cookies| 3.707070707070707|\n",
      "|            honduran|3.7982456140350878|\n",
      "| unprocessed-freezer| 3.824915824915825|\n",
      "|fillings-and-fros...|3.9347826086956523|\n",
      "|            birthday| 3.944741532976827|\n",
      "|             jellies|3.9685714285714284|\n",
      "|          water-bath|3.9867256637168142|\n",
      "|served-hot-new-years|               4.0|\n",
      "|      pork-crock-pot|               4.0|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tagsWorstReviews: Unit = ()\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tagsWorstReviews = recipes\n",
    "    .select($\"id\" as \"recipe_id\", explode(split(expr(\"substring(tags, 2, length(tags) - 1)\"), \", \")) as \"tag\")\n",
    "    .withColumn(\"tag\", trim($\"tag\", \"' []\"))\n",
    "    .join(interactions.select(\"recipe_id\", \"rating\"), Seq(\"recipe_id\"))\n",
    "    .groupBy(\"tag\")\n",
    "    .agg(avg(\"rating\") as \"avg_rating\")\n",
    "    .orderBy(\"avg_rating\")\n",
    "    .show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Найдите contributor'а, который прислал:\n",
    "    1. Наиболее используемый рецепт\n",
    "    2. Наиболее высоко оцененный рецепт"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`A.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mostUsedRecipe: Array[org.apache.spark.sql.Row] = Array([2886,1613])\n"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mostUsedRecipe = interactions\n",
    "    .groupBy(\"recipe_id\")\n",
    "    .agg(count(\"*\") as \"num_of_uses\")\n",
    "    .orderBy(-$\"num_of_uses\")\n",
    "    .take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|contributor_id|recipe_id|\n",
      "+--------------+---------+\n",
      "|          1762|     2886|\n",
      "+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recipes\n",
    "    .filter($\"id\" === mostUsedRecipe(0)(0))\n",
    "    .select($\"contributor_id\", $\"id\" as \"recipe_id\")\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`B.` В данных много рецептов, в которых средний рейтинг рецепта равен 5.0 (их больше 100000). Поэтому еще посчитаем количество отзывов на рецепты, отфильтруем те, у которых рейтинг равен 5.0, и будем считать наиболее высоко оцененным рецептом тот, у которого рейтинг 5.0 и максимальное количество отзывов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mostRatedRecipe: Array[org.apache.spark.sql.Row] = Array([55309,5.0,52])\n"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mostRatedRecipe = interactions\n",
    "    .groupBy(\"recipe_id\")\n",
    "    .agg(avg($\"rating\") as \"avg_rating\", count(\"*\") as \"num_of_uses\")\n",
    "    .filter($\"avg_rating\" === 5.0)\n",
    "    .orderBy(-$\"num_of_uses\")\n",
    "    .take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|contributor_id|recipe_id|\n",
      "+--------------+---------+\n",
      "|         63098|    55309|\n",
      "+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recipes\n",
    "    .filter($\"id\" === mostRatedRecipe(0)(0))\n",
    "    .select($\"contributor_id\", $\"id\" as \"recipe_id\")\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Опишите процесс работы broadcast join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Насколько я понял, broadcast нужен для того, чтобы можно было эффективно выполнять join-ы больших датафреймов с небольшими. То есть Spark может распространить маленький датафрейм на все ноды кластера, а затем уже join может быть выполнен более эффективно на каждом из executor-ов между соответствующими партициями большого датафрейма и всем маленьким датафреймом (тогда требуется минимальный shuffle данных)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Опишите процесс работы cache и причины его использования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cache нужен для переиспользования вычислений, которые предшествуют текущему. Потенциально это может ускорить другие запросы, использующие эти же данные. В DataFrame API есть два метода `cache()` и `persist()`, которые могут использоваться для кэширования. Различие в том, что во втором методе есть возможность выставить storageLevel, который будет указывать, где хранить эти данные (дефолтное значение — MEMORY_AND_DISK). Кэширование является lazy transformation, поэтому сразу после вызова функции с данными ничего не происходит, но query plan обновится с помощью добавления оператора InMemoryRelation. Spark будет искать данные из кэша и считывать их оттуда, если они доступны. Если он не найдет данные в кэше (например, при первом запуске запроса), то он сохранит эти данные там и будет использовать их сразу же после этого."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
